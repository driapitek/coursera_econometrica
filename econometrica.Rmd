---
title: "Untitled"
output: html_document
editor_options: 
  chunk_output_type: console
---


# Неделя один

## Обозначения

* Одна зависимая (объясняемая) переменная: *y*

* Несколько регрессоров (предикторов, объясняющих переменных): *x, z...*

* По каждой переменной n наблюдений: $y_1, y_2...y_n$

Модель --- это формуля для объясняемой переменной.

### Пример

Возьмём например данные по машинам 1920-х годов. Тут видимая линейная взаимосвязь.

![Данные по машинам 1920-х гдов](img/car.PNG)

Модель для этих данных может иметь вид $y_i = \beta_1 + \beta_2x_i +  \varepsilon_i$

Что здесь есть что? У нас есть наблюдаемые переменные, 

* *y* — это длина тормозного пути

* *x* — это скорость, с которой ехала машина. 

* Есть неизвестные коэффициенты $ \beta_1, \beta_2$

* случайная составляющая, ошибка

То есть, $\beta_2$ показывает — насколько увеличивается тормозной путь, если машина разгонится на один лишний километр в час. И есть некая случайная составляющая $ε_i$, это может быть все что угодно:

  * водитель по-другому нажимал на тормоз, 
  * что-то там на дороге было другое, 

то есть это та часть, по которой у нас нет возможности предсказать, но, тем не менее, вот эта случайная ошибка она входит в y.

### План действий

* Придумать адекватную модель

* Получить оценки неизвестных параметров $\widehat\beta_1, \widehat\beta_2$

* Спрогнозировать, заменив неизвестные параметры на оценки $\widehat y_i = \widehat\beta_1 + \widehat\beta_2 x_i$

## Метод наименьших квадратов

Как найти  $\beta_1, \beta_2$? Собственно методом наименьших квадратов.

Если мы придумали какие-то оценки, $\beta_1, \beta_2$ то соответственно возникает такое понятие как ошибка прогноза

$$\widehat  \varepsilon_i = y_i - \widehat y_i$$

Есть суммарная ошибка, чтобы суммарная ошибка не занулялась одна в плюс, другая в минус, не компенсировали друг друга, мы возведем в квадрат. И посчитаем сумму квадратов ошибок прогноза, то есть сумма $\widehat  \varepsilon_i ^2$

$$Q(\widehat\beta_1, \widehat\beta_2) = \sum_{i=1}^{n} \widehat\varepsilon^{2}_{i} =  \sum_{i=1}^{n} (y_i - \widehat y_i)^2 $$


*Суть метода МНК*

Возьмите в качестве оценок такие коэффициенты $\widehat\beta_1, \widehat\beta_2$ при которых сумма квадратов ошибок прогноза будет минимальна

### Случай для одного регрессора

Всё сводится к тому, что в модели $$M_1: y_i = \beta + \varepsilon_i$$

$$\widehat\beta = \bar y$$

![Случай для одного регрессора](img/sample1.PNG)

### Случай для двух регрессоров

Тут несколько сложнее, подробности на скриншоте

![Случай для двух регрессоров](img/sample2.png)

### Случай для множества регрессоров

Что мы получили:

![Случай для двух регрессоров](img/resume.png)

Ещё раз пробежимся по терминам

![Случай для двух регрессоров продолжение](img/resume2.png)

### Графическое представление

Изобразим на графике, всё то, о чем мы только что проговорили.

![Графический вывод](img/resume3.png)

В частности $\widehat\beta_1$, т.е. точка на оси ординат от пересечении с прямой регрессии, называют пересечением или Intercept-ом

Метод наименьших квадратов подбирает прямую так, чтобы суммарные расстояния от точек до прямой были минимальными.

![Графический вывод продолжение](img/resume4.png)

### Множественные регрессоры

Случай множественных регрессоров принципиально не отличается от двух регрессоров. Поэтому рассмотрим на примере трёх предикторов.

![Множественные регрессоры](img/resume5.png)

### Суммы квадратов

![Суммы квадратов](img/ss.png)

Что есть что:

* RSS (Residual Sum of Squares) --- этот показатель меряет, насколько велики эпсилон с крышкой, насколько они далеко лежат от нуля.

* TSS (Total Sum of Squares) ---  этот показатель меряет, насколько каждый из $y_i$ не похож на $y$ среднее. Если $y_i$ далеко лежит от $y$ среднее, соответственно, это слагаемое в сумме квадратов будет большим. И вся сумма квадратов будет большой. 

* ESS (Explained Sum of Squares) --- она показывает, насколько прогнозное значение $y_i$ с крышкой далеко легло от $y$ среднего. 

## Лекбез по линейной алгебре

Язык эконометрики во многом — это язык линейной алгебры, нужно его знать.

### Обозначения

![Обозначения](img/symbols.png)

* Маленькой буквой $y$ мы будем обозначать вектор, то есть столбик из всех игреков, записанных друг под другом: у_1, у_2 и так далее, у_n. 

* Ну, соответственно, $х$ маленькое — это все иксы: х_1, х_2 и так далее, х_n, записанные друг под другом. 

* Аналогично $\widehat\beta$. 

* И еще введем обозначение: единичку со стрелочкой. Это будет вектор-столбец, то есть столбик из единичек в количестве n штук, потому что у нас в модели будет n наблюдений. 

Тогда для нашей модели

$$\widehat y = \widehat\beta_1 \cdot \overrightarrow 1 + \widehat\beta_2 \cdot x + \widehat\beta_3 \cdot z$$

* Большая буква *X* --- это все регрессоры, помещенные в одну большую табличку чисел, которые называются матрицей. 

![Таблица регрессоров](img/symbols2.png)

* Рассмотрим ещё такое понятие как длина вектора

![Длина вектора](img/vector.png)

Где у нас бывают длины векторов: 

![Пример длины вектора](img/vector1.png)

Есть одно замечательное применение. Если скалярное произведение векторов равно нулю, значит они перпендикулярны.

![Скалярное произведение векторов](img/vector2.png)

```{r}
x <- c(1,2, -3)
y <- c(-6,0,-2)

sum(x * y)
```

### Линейная модель регрессии в n-мерном пространстве

![n-мерное простарнство](img/nmernoe.PNG)

Есть вектор *y*, есть вектор из одних 1, я продолжаю этот вектор до прямой и оказывается, что 

* в простой модели без регрессоров спроецировав вектор *y* на эту прямую, я получу вектор *y* с крышкой --- предсказанные значения. 

Соответственно, мы получили попутно еще один факт: если любой вектор проецировать на вектор из 1, то получится вектор средних значений

## Геометрия множественной регрессии

Напомню, что мы вывели условие первого порядка

![Геометрическая интерпретация условий первого порядка](img/nmernoe2.PNG)

Это означает, что вектора перпендикулярны.

Облачко это все те вектора, которые можно получить, складывая с некоторыми весами вектор x, вектор z и вектор из единичек. Это гиперплоскость.

Мы получаем следующую интрпретацию метода наименьших квадратов:

  1. Первый геометрический факт
  Прогнозы, которые мы получаем по методу наименьших квадратов --- это проекция исходного вектора зависимых переменных *y* на множество векторов, получаемых с помощью сложения с разными весами вектора из единичек, вектора *x* и вектора *z*.
  
  2. Второй геометрический факт
  Если я спроецировать вектор *y* на прямую, порождаемую вектором из единичек, и спроецировать *y* с крышечкой на ту же самую прямую, то эти проекции попадут в одну и ту же точку.
  
  3. $TSS = ESS + RSS$
  
  4. $\frac{ESS}{TSS}= \frac{BC^2}{AB^2}=(\frac{BC}{AB})^2= (cos \varphi)^2$
  
![Геометрчиеские факты](img/nmernoe3.PNG)

## Коэффициент детерминации

Если в регрессию включён свободный член $\beta_1$ то действуют следующие правила:

![Правила при включении свободного члена в регрессию](img/beta1.PNG)

Эти правила позволяют придумать простой показатель качества работы модели --- коэффициент детерминации.

Чем прогнозы точнее похожи на настоящие значения, тем меньше будут ошибки прогнозов и тем меньше будет сумма квадратов ошибок прогнозов RSS. Соответственно $\frac{ESS}{TSS} = R^2$ будет примерно равен 1 если RSS  будет у ноля. Соответственно мы получим коэффициент детерминации, который всегда лежит от 0 до 1. 

Другими словами --- коэффициент детерминации это доля объяснённого разброса в общем разбросе.

![Правила при включении свободного члена в регрессию](img/beta2.PNG)

С одной стороны, коэффициент детерминации --- это доля объясненной дисперсии игрек, доля объясненного разброса. 

С другой стороны, коэффициент детерминации --- это выборочная корреляция между прогнозами и настоящим игрек, взятое в квадрат. 

Чем коэффициент детерминации выше, тем больше предсказание похожи на реальные значения. 
Чем коэффициент детерминации выше, тем выше доля объясненной дисперсии. 

#### Пример с фертильностью. МНК в R

Посмотрим как зависит фертильность от других 

```{r}
t <- swiss
# нарисовать много диаграм рассеивания
# install.packages("GGally") # матрица диаграмм рассеяния
library("GGally")
str(t)
ggpairs(t)

```

Уже тут можно посмотреть много всяких корреляций.

Перейдём к оценке

```{r}
model2 <-  lm(data = t, Fertility ~ Agriculture + Education + Catholic)
# КОэффициенты
coef(model2)

# Спрогнозированные значения
fitted(model2)
# Остатки
residuals(model2)
# ПОказатель RSS
deviance(model2)
# Показатель R квадрат
report <- summary(model2)
report$r.squared

# Это тоже самое что
cor(t$Fertility, fitted(model2))^2
```


# Неделя два

Вторая неделя --- стат.свойства оценок коэффициентов

Формулируем стандартные предпосылки --- они ведут к свойствам --- свойства позволят строить доверительные интервалы.

Для формулировки предпосылок --- сформулируем поняти условное мат.ожидание.

## Условное математическое ожидание

![Условия мат. ожидания](img/expected_value.PNG)

Если формально: это некая случайная величина s с тильдой, которая является, с одной стороны, функцией от r, и, с другой стороны, эта самая s с тильдой очень похожа на s, а именно: математической ожидание от s с тильдой равно математическому ожиданию от s, и ковариация между s и любой функцией от r равна ковариации между s с тильдой и той же самой функцией от r. 

То есть получается, что s с тильдой и s очень похожи, s с тильдой и s невозможно отличить, если смотреть только на математическое ожидание или на ковариацию с r, с r-квадрат, с любой функцией от r

Условное мат. ожидание --- это Верно! $E(y_i | x_i)$  это наилучший прогноз $y_i$ формулируемый с помощью $x_i$.

На практике 

![Условия мат. ожидания](img/expected_value2.PNG)

### Пример

Здесь показано как рассчитывать

![Условия мат. ожидания](img/expected_value3.PNG)

## Условное дисперсия

Если величины непрерывны и есть совместная функция плотности

![Условия мат. ожидания](img/ualovna.PNG)

### Свойства условного ожидания

![Условия мат. ожидания](img/ualovna2.PNG)

Найдите $E(x_i^2+2x_i | x_i)$ это  $x^{2}_i+2x_i$. Потому что если мы знаем $x_i$, то и всё выражение слева мы легко спрогнозируем.

### Условная дисперсия и условная ковариация

![Условия мат. ожидания](img/ualovna3.PNG)

Свойства:

![Условия мат. ожидания](img/ualovna4.PNG)

*Пример *

Упростите $Var(x^{2}_i+2x_i+\varepsilon_i|x_i)$ это равно $Var(\epsilon_i | x_i)$ Потому что 
Вспомним свойство $Var(s+h(r) | r)=Var(s|r)$ В роли s выступает $\varepsilon_i$. А если интуитивно: хотим спрогнозировать выражение слева, зная $x_i$ часть $x_i^2+2x_i$ нам доподлинно известна и на точность прогнозирования (а именно её меряет условная дисперсия) не влияет.

## Геометрическая иллюстрация условного мат.ожидания

![-](img/geom_intrpret.png)

Запомнить

* Интерпретируем дисперсию как квадрат длины случайной величины, 

* Интерпретируем корреляцию между случайными величинами как косинус угла между ними.

С помощью условного математического ожидания мы сформулируем стандартные предпосылки на случайную составляющую $\varepsilon$, а миеннто три предпосылки

* Математическое ожидание от каждой случайной составляющей при известных иксах, то есть при известном каждом регресссоре для каждого наблюдения, я пишу коротко матрицу X. Условное математическое ожидание от $\varepsilon_i$ при всех известных регрессорах равна 0

* Условное математическое ожидание от $\varepsilon^2$ при всех условных регрессорах равна $\sigma^2$. Или можно точно так же сказать, что условная дисперсия $\varepsilon_i$  при известной матрице X равна $\sigma^2$.

* Kовариация между $\varepsilon_i$  и $\varepsilon__j$  при фиксированной матрице X равна нулю.

![Предпосылки](img/predpos.png)

Предпосылки про ковариацию и дисперсию можно коротко записать с помощью такого понятия как ковариационная матрица

![Предпосылки](img/covar.png)

Когда мы говорим «ковариационная матрица некоего случайного вектора», мы имеем в виду здоровую табличку чисел. Первое число в первой строке — это дисперсия  $\varepsilon_1$ . Второе число в первой строке, то есть первая строчка, второй столбец, (1,2) координаты, — это ковариация  $\varepsilon_1$  и  $\varepsilon_2$. 

Соответственно, в ковариационной матрице, скажем, в третьей строчке, втором столбце находится ковариация  $\varepsilon_3$  и  $\varepsilon_2$, это третья строчка, второй столбец. С

оответственно, в этой матрице находятся и все дисперсии каждого  $\varepsilon$ , и все попарные ковариации:  $\varepsilon_1$  с  $\varepsilon_3$ ,  $\varepsilon_2$  с  $\varepsilon_7$ ... все-все-все ковариации и дисперсии находятся в одной матрице, в одной табличке чисел.


Соответственно, первые наши две предпосылки на ε можно как сформулировать?
Что ковариации равны нулю, а на диагонали дисперсии равны $\sigma^2$. То есть матрица принимает такой простой вид. 

![Предпосылки](img/covar2.png)

Соответственно, в нашем случае наши предпосылки можно записать как ковариационная матрица вектора $\varepsilon$ при фиксированных регрессорах X равна $\sigma^2$ умножить на эту самую единичную матрицу, которая обозначается буковкой I, сокращение от английского «identity». 

### Итоги

* У нас есть предпосылки, что дисперсия $\varepsilon$ при фиксированных регрессорах X равна $\sigma^2$ умножить на единичную матрицу, что на самом деле просто означает, что дисперсия $\varepsilon_i$ при фиксированном X равна $\sigma^2$, а ковариация разных  $\varepsilon$  при фиксированном X равна нулю.

* И у нас есть линейная модель,

![Итоги](img/covar3.png)

Это для примера двух объясняющих переменных. И, соответственно, эти предпосылки позволяют посчитать дисперсию любой оценки МНК $\beta_j$ с крышкой и любую ковариацию $\beta_j$ с крышкой и $\beta_l$ с крышкой. 

Мы посчитаем для начала дисперсию, условную, оценки метода наименьших квадратов и ковариацию оценки метода наименьших квадратов для случая парной регрессии.

![Итоги](img/parnaya_regression.png)

## Условная дисперсия МНК оценок. Доказательство

![Условия нашей задачи](img/mnk.PNG)

Что мы получаем. Запишем примечания

![Примечания](img/mnk2.PNG)

Посчитаем в рамках предположений ковариацию $\widehat\beta_2$  и среднее

ПОлучаем не что иное как

![Примечания](img/mnk3.PNG)

Итого в парной регрессии мы имеем

![Итого в парной регрессии](img/pars_reg.png)

Если регрессор $z$ сильно коррелирован с другими регрессорами, то Величина $RSS_z$ будет примерно равна 0 и поэтому дисперсия $Var(\widehat\beta_z|X)$ будет большой


![Теорема](img/vars8.png)

Штрих --- это транспонированная матрица.


![Свойства](img/linal.png)

### Доказательство формулы для ковариационной матрицы

Этот фрагмент особо полезен тем, кто знает линейную алгебру. Оказывается, средствами линейной алгебры легко не только сразу посчитать дисперсию $\widehat\beta_1$ с крышкой или $\widehat\beta_2$ с крышкой, одним махом можно найти все эти дисперсии и все ковариации.

Свойства ковариационных матриц

![Свойства](img/features.png)

В этой матрице находятся и дисперсии условные каждого  $\widehat\beta$, и ковариации каждого $\widehat\beta$ с каждым $\widehat\beta$. И в скалярном виде мы их выводили долго по отдельности и только для случая парной регрессии, а в матричном виде они выводятся легко, все и сразу, и этой формулой можно будет пользоваться.

![Свойства](img/matrix.png)

## Оценка ковариационной матрицы

Константа $\sigma^2$ неизвестна, и именно эта константа входит в формулу для условной дисперсии любого оценённого коэффициента. А нам хочется строить доверительные интервалы для коэффициентов, проверять гипотезы, поэтому нам нужен какой-то способ оценить неизвестную константу.

![Свойства](img/vcov.png)

## Статистические свойства оценок коэффициентов

Оценки коэффициентов, которые мы получаем методом наименьших квадратов, обладают рядом замечательных статистических свойств. Эти свойства, их очень много, и поэтому, чтобы как-то их было проще все осознать, мы их поделим на три части

![Свойства](img/boshos.png)

Предпосылки для применения метода наименьших квадратов и для получения хороших свойств коэффициентов.

Предпосылки

![Свойства](img/boshos2.png)

Предположения на $\varepsilon_i$

![Свойства](img/gomoskedos.png)

Предпоссылки на регрессоры

![Свойства](img/predpos_on_regressors.png)

Когда все эти предпосылки, которые мы сформулировали: 

  * предпосылки о зависимости y от x, 
  * предпосылки о распределении $\varepsilon$, 
  * предпосылки о регрессорах, 

когда все они выполнены, мы получаем, что верны три группы свойств.

### Базовые

![Свойства](img/basa.png)

Это очень хорошее свойство. Это говорит о том, что наш метод, конечно, может ошибаться, и оценка $\widehat\beta$, которую мы получаем, может не совпадать с настоящим $\beta$, но $\widehat\beta$ иногда будет больше настоящего $\beta$, иногда будет меньше настоящего $\beta$, но в среднем,$\widehat\beta$ попадает то влево, то вправо, но в среднем попадает в неизвестный коэффициент $\beta$

![Свойства](img/basa2.png)

Это очень хорошее свойство. Оно говорит о том, что если вы хотите простую оценку, то есть линейную, хотите оценку несмещенную, которая в среднем бы попадала в неизвестную истину, то ничего лучше оценок метода наименьших квадратов у нас не получится. То есть математически это означает, что условная дисперсия$\widehat\beta$ при фиксированных регрессорах альтернативного больше либо равна, чем условная дисперсия $\widehat\beta$, полученная по методу наименьших квадратов опять же при фиксированных регрессорах. 

![Свойства](img/basa3.png)

То есть это свойство говорит, что оценка $\widehat\sigma^2$, предложенная нами, — RSS, делённая на (n – k), — она тоже несмещённая, и она несмещённым образом оценивает $\sigma^2$.

### Асимптотические

Это те, что предполагают что число экспериментов велико

![Свойства](img/asimptota.png)

### Требующие нормальности

![Свойства](img/normal.png)

## Построение доверительных интервалов и проверка гипотез

Проверять гипотезы можно в двух случаях 

* Число наблюдений велико 

* Случайные ошибки нормальны

### Построение доверительного интервала

![Свойства](img/svoi.png)

С ростом стандартной ошибки ширина доверительного интервала для коэффициента увеличивается


## Проверка гипотез

Нулевая гипотеза может только отвергаться или не отвергаться

#### Описание любого теста

* Предпосылки теста

* $H_0$ --- проверяемая гипотеза

* $H_1$ или $H_a$ --- альтернативная или другими словами конкурирующая гипотеза

* Формула для вычисления статистики

* Закон распределения статистики при верной $H_0$

#### Последовательность действий

1. Формулируем гипотезу $H_0$ и выбираем уровень значимости. Это вероятность ошибки первого рода или вероятность отвергнуть $H_0$ при условии, что она верна. $\alpha = P (отвергнуть H_0| принять H_0)$. Можно формулировать гипотезу и под неё собирать данные

2. Рассчитываем наблюдаемое значение тестовой статистики $S_obs$

3. Находим критическое значение тестовой статистики $S_cr$

4. (a) Сравниваем $S_obs$ и $S_cr$ и делаем вывод о $H_0$ --- устаревший вариант. Сейчас всё чаще делают так:

4. (b) Сравниваем P-значение и $\alpha$, делаем вывод о $H_0$. Если P-значение больше чем уровень $\alpha$, то гипотеза не отвергается. Если полученной P-значение меньше, то гипотеза отвергается

### Пример. Доверительный интервал для коэффициента бета

считается это всё вот так

![Пример](img/primer.png)

`qt()` --- это функция в R

### Пример. Доверительный интервал для дисперсии

Мы опираемся на теорему RSS делённое на сигма квадрат

`qchisq()` --- это функция в R

![Пример](img/primer2.png)

### Пример. Проверка гипотезы о коэффициенте beta

Предпологаем что коэффициент при доле сельскохозяйственного мужского населения равен нулю --- другими словами фертильность не зависит от показателя того, насколько этот регион является сельскохозяйственным. Есть три способа проверить эту гипотезу 

![Пример](img/primer3.png)

В литературе, скажем в научных статьях, очень часто стандартные ошибки выписывают под коэффициентами.



### Интерпретация стандартной таблички

При оценке линейной модели регрессии, любой статистический пакет выдает более-менее стандартную табличку. Вот такую табличку выдает R.

![Пример](img/table.png)

1. Первый столбик в ней это, собственно, оценки коэффициентов.
Т.е. смысл этого столбика, что мы можем записать уравнение линейной регрессии по используя эти коэффициенты.

2. Второй столбик --- это стандартные ошибки. Это корни из диагональных элементов ковариационной матрицы

3. Третий столбик --- это компьютер автоматом проверяет гипотезу о том, что на самом деле, зависимости от данной переменной нет. Он делает это при помощи Т-статистики. Т.е. это первый столбец делить на второй. T-статистика, проверяющая гипотезу о незначимости коэффициента может принимать любое значение. Знак T-статистики определяется знаком оценки коэффициента, и по модулю она может быть произвольной

4. P-value

## Особенности проверки гипотез

* Если $H_0$ не отвергается, это говорит о том, что зависимости нет. 

* Надо говорить аккуратно: «$H_0$ не отвергается» — это означает, что данные не противоречат гипотезе $H_0$

* Значимость и существенность. Значимость это статистический факт --- равен коэффициент нулю или не равен. Существенность --- это на сколько он не равен нулю.

* Стандартизировать коэффициенты перед анализом. Чтобы получить одинаково интерпритируемые единицы измерения

* К сожалению, очень часто распространена такая порочная практика, что исследователь берет, включает кучу, не задумываясь о теоретической модели, включает кучу объясняющих переменных в свою модель и выбирает те из них, которые по t-статистикам оказались значимы. Это подход неправильный, поскольку как только мы согласились на некую вероятность ошибки первого рода, например, мы выбрали вероятность ошибки первого рода типичную в экономических приложениях 5%. «Я запустил регрессию на кучу переменных и отобрал те, которые значимы» — это неправильный подход. 


## Проверка гипотезы о связи коэффициентов

К примеру, мы хотим в рамках нашей модели проверить, что воздействие, рост фертильности, вызванный увеличением доли мужчин, занятых в сельском хозяйстве, одинаков по силе с ростом фертильности при росте католического населения, то есть я хочу проверить гипотезу о том, что разница этих двух коэффициентов равна нулю. Как это сделать?

![Гипотеза не отвергается](img/ne_otverget.png)

Второй способ --- подобрать коэффициенты. Т.е. отнять и прибавить коэффициент при втором регрессоре. Программа автоматом рассчитает вероятность того, что разница коэффициентов равна нулю

![Гипотеза не отвергается](img/tom2.png)


## Задания на R

Подключаем нужные пакеты

```{r}
library(memisc)
library(tidyverse)
library(psych)
library(lmtest)
library(sjPlot)
library(sgof)
library(foreign)
library(car)
library(hexbin)
library(rlms)
```

### Работа со случайными величинами

* Генерация случайных величин. 

100 величин, распределение нормально.
Любое распределение генерится в R функцией, которая начинается с *r* и далее название распределения.


```{r}
z <- rnorm(100, mean = 5, sd = 3)
z[56]
z[2:9]
qplot(z)
```

* Построим функцию плотности

Все функции плотности начинаются с буквы *d* (от density) в R.
```{r}
x <- seq(-10, 15, by = 0.5)
y <- dnorm(x, mean = 5, sd = 3)
qplot(x, y, geom = "line")
```

* Расчёт вероятностей

Все функции плотности начинаются с буквы *p* (от probability) в R.

```{r}
pnorm(3, mean = 5, sd = 3)
```

если я хочу найти вероятность того, что z лежит в диапазоне от 4 до 9, то это есть с точки зрения здравого смысла вероятность того, что z меньше 9 минус вероятность того, что z меньше 4

```{r}
pnorm(9, mean = 5, sd = 3) - pnorm(4, mean = 5, sd = 3)
```

* Квантили распределения. $P(Z<a)=0.7$ какое $a$

Все функции плотности начинаются с буквы *q* (от quantile) в R.

```{r}
qnorm(0.7, mean = 5, sd = 3)
```

Квантиль — это на самом деле, обратная функция к функции распределения. То есть, если, например, я хочу найти такое число а, чтобы вероятность того, что z меньше а была равна, скажем, 0.7. И вот надо найти такое а. То соответственно, это можно найти с помощью квантильной функции

Есть разные популярные распределения. chisq --- хи квадрат. t --- стьюдента. f --- Эф распределение

### Проверка гипотез о распределениях 

Множественная регрессия, проверка гипиотез

```{r}
h <- swiss
glimpse(h)
```

Мы оценим линейную модель регрессии. Будем предполагать, что фертильность, Fertility, зависит  от доли католического населения в данном кантоне, от показателя, насколько это регион сельскохозяйственный, и, скажем, от Examination.


```{r}
model <- lm(data = h, Fertility ~ Catholic + Agriculture + Examination)
```

Посмотрим отчет по этой моделе

```{r}
summary(model)
```

Отсюда можем сказать,  что 
гипотеза о том, что $\beta_1$ равно нулю, отвергается; 
гипотеза о том, что $\beta_2$ равно нулю, не отвергается; 
гипотеза о том, что $\beta_3$ равно нулю, не отвергается;
гипотеза о том, что $\beta_4$ равно нулю, отвергается.

Только значения коэффициентов, И также можем легко получить доверительные интервалы:

```{r}
coeftest(model)
confint(model)
```


* Проверка линейных гипотез

проверим линейную гипотезу о том, что коэффициент зависимости при доле католического населения и при доле населения, занятого в сельском хозяйстве, одинаковые.

Воспользуемся хитрым способом, в котором мы складываем две переменные --- способ с построением вспомогательной регрессии --- чтобы проверить гипотезу.

Значок `I` означает инструкцию для R, что надо трактовать `Catholic + Agriculture` — «плюс» в прямом смысле.

```{r}
model_aux <- lm(data = h, 
                Fertility ~ Catholic + I(Catholic + Agriculture) + Examination)
```

Выведем отчёт о модели

```{r}
summary(model_aux)
```

Коэффициент при Catholic незначим, потому что P-уровень равен `0.158483`. Это говорит о том, что гипотеза о том, что коэффициенты при Catholic и Agriculture равны, не отвергается.

Таким образом, мы смогли проверить гипотезу о том, что два коэффициента истинных, неизвестных нам, равны, и эта гипотеза в нашем случае не отвергается.

### Стандартизированные коэффициенты и эксперимент с ложно-значимыми регрессорами

Один из способов почувствовать существенный коэффициент или нет, — это посчитать стандартизированные коэффициенты $\widehat\beta$, то есть привести все объясняющие переменные и объясняемую переменную к неким универсальным единицам измерения, чтобы они были сравнимы, а именно: вычесть из каждой переменной её среднее и поделить на оценённое стандартное отклонение.

Шаг 1 --- Стандартизация коэффициентов

На шаге один мы преобразуем каждую переменную, масштабируем каждую переменную. Значит, создадим набор `h_st`, где мы изменим каждую переменную, функция называется `mutate_each`, в наборе данных `h`, а формула, по которой мы будем, функция, по которой мы будем менять каждую переменную, называется `scale`, она осуществляет как раз масштабирование, вычитание среднего и деления на стандартную ошибку.

```{r}
h_st <- mutate_each(h, "scale")
glimpse(h_st)
```

Шаг 2 --- построение модели

```{r}
model_st <- lm(data = h_st, Fertility ~ Catholic + Agriculture + Examination)
summary(model_st)
```

Шаг 3 --- визуализация коэффициентов. 

Шаг 4 --- принятие решения

#### Искусственный эксперимент

Сейчас мы на искусственных данных проиллюстрируем идею того, что нельзя просто так построить регрессию и отвечать на вопрос, а какие коэффициенты у меня значимы. 

Мы сочиним в нашем искусственном эксперименте совершенно несвязанный игрек, который никак не зависит от якобы объясняющих переменных. У нас будет 40 якобы объясняющих переменных, одна якобы зависимая, хотя на самом деле независимая, и мы будем строить, оценивать модель линейной регрессии. 

Генерим данные

```{r}
set.seed(42)
d <- matrix(rnorm(100*41, mean = 0, sd = 1), nrow = 100)
df <- data.frame(d)
glimpse(df)
```

Будем объяснять первую переменную, всеми оставшимися в наборе данных переменными. Для этого есть удобное сокращение в виде точки `.` 

```{r}
model_pusto <- lm(data = df, X1~.)

summary(model_pusto)
```

У нас получилось, что переменные X3 и X35 ылияют на первую переменную при 5% уровне значимости, а X19 и X32 влияют на 10% уровне значимости. 

С чем это связано? Это связано с тем, что когда мы фиксируем уровень значимости, мы соглашаемся на некоторую вероятность ошибиться.

Соответственно, когда мы зафиксировали уровень значимости 10%, это означает, что с вероятностью 10% мы в случае на самом деле отсутствия зависимости якобы её обнаружим.

#### Вывод

Соответственно, из этого искусственного эксперимента нужно сделать простой вывод, что стратегия «я оценю модель с большим количеством объясняющих переменных и выпишу из них значимые, и скажу, что от них игрек зависит», — неправильная, потому что в силу того, что есть для каждого регрессора десятипроцентный или пятипроцентный шанс сделать ошибку, при большом количестве регрессоров кто-то якобы значимым будет, даже если на самом деле никакой зависимости нет.

### Сравнить несколько моделей

Есть несколько моделей 

```{r}
model <- lm(data = h, Fertility ~ Catholic + Agriculture + Examination)
model2 <- lm(data = h, Fertility ~ Catholic + Agriculture)
```

Как нам лучше их сравнить. Сделаем табличку, сравниваем при помощи функции `mtable()` из пакета `memisc`

```{r}
compare_12 <- mtable(model, model2)
```

# Неделя 3

## Прогнозирование во множественной регрессии

Первый сюжет — это прогнозирование, как строить прогнозы и, более интересно, как строить доверительные интервалы для прогнозов. 
И второй сюжет — это как выбрать наилучшую модель.

*Прогнозирование.* 

Есть некоторая теоретическая модель, на примере двух объясняющих переменных:

![_](img/prognoz.PNG)

соответственно, оценив неизвестные коэффициенты методом наибольших квадратов, мы получаем оценённую регрессию, которая нам позволяет делать точечные прогнозы.

Но интересно не точечные прогнозы построить, а интересно построить доверительный интервал. Интересно, вот мы, хорошо спрогнозировали, что завтра будет минус 20 градусов: это от минус 21-го до минус 19-ти? Или это от минус 40-ка до нуля?

*Интервальное прогнозирование*

Точность прогноза определяется шириной доверительного интервала. 

Допустим, я хочу построить прогноз для человека с заданным ростом, скажем, с ростом 170 сантиметров для мужчины. И тут возникает два варианта: 

* я могу строить доверительный интервал для веса среднестатистического мужчины с ростом 170 сантиметров, 

* а могу строить доверительный интервал, который называется предиктивный интервал, для конкретного мужчины с ростом в 170 сантиметров. 

В чём разница?

В первом случае средний вес всех мужчин, которые имеют рост 170 сантиметров — это какая-то константа. 
Вот эта вот одна константа — средний рост всех мужчин. Я его не знаю, и я имею выборку наблюдений, могу попытаться его спрогнозировать. Соответственно, ошибка прогноза, которая возникает, она связана только с тем, что у меня данные не по всем мужчинам Земли, а у меня данные только по небольшой случайной выборке. А вторая задача, если я пытаюсь спрогнозировать рост конкретного мужчины, который вот сейчас войдёт в комнату, и у него рост 170 сантиметров. Если я пытаюсь спрогнозировать его вес, то здесь источников ошибки два. Первый источник ошибки — это то, что я руководствовался при оценке модели выборкой, а второй источник ошибки состоит в том, что это не среднестатистический мужчина, рост которого равен константе, а это случайный мужчина ростом 170 сантиметров. *Поэтому во втором случае два источника ошибки, и предиктивный интервал будет шире. *

Оценки дисперсии

![_](img/ocenka.PNG)

Подходить к оценке доверительного интервала можно двумя способами: либо нужно иметь большое количество наблюдений, либо нужно верить в нормальный закон распределения $\varepsilon_i$ при фиксированных иксах.

![_](img/ocenka2.PNG)

Предиктивный интервал --- аналогично

![_](img/ocenka3.PNG)

Предиктивный интервал для $y_i$ шире.  Дисперсия ошибки прогноза $y_i$ больше чем дисперсия ошибки прогноза $E(y_i|X)$ на $\sigma^2$.

Ещё раз о терминологии: 

* когда говорят «доверительный интервал», обычно имеют в виду доверительный интервал для среднего значения зависимой переменной, 
 
* когда говорят «предиктивный интервал», обычно имеют в виду интервал для значения зависимой переменной для конкретного наблюдения.

### Пример построения интервалов для прогнозов

Исследователь оценил по 2040 наблюдениям модель стоимости квартир в Москве и получил следующие результаты. 
Оценка цены квартиры равна минус 62 плюс 2.6 умножить на общую площадь 
Оценка неизвестной дисперсии $\varepsilon$ $\widehat\sigma^2 = 1154$. Известна так же оценка для неизвестных коэффициентов матрица $\widehat{Var}(\widehat{\beta}|X)$. Наша цель — мы хотим спрогнозировать стоимость квартиры площадью 60 метров

мы для данных регрессоров, для данного значения регрессоров хотим посчитать 

a) прогноз точечный price F с крышечкой и хотим посчитать два интервала. 

b) Один 95 %-ный доверительный интервал для неизвестной средней стоимости квартиры с площадью 60 метров. 

c) И хотим посчитать 95 %-ный предиктивный, предиктивный интервал для фактической стоимости одной случайно выбираемой квартиры опять же с площадью 60 метров. 

Еще раз, в чем разница между этими объектами b и с?  
b --- это средняя стоимость квартиры в Москве с площадью 60 метров.
c --- это стоимость случайно выбираемой квартиры в Москве с площадью 60 метров. 

Перейдем к построению доверительного интервала, у нас в каждом случае, в случае b и в случае c, есть ошибка прогноза.

В первом случае --- это на сколько наша спрогнозированная цена отличается от средней стоимости по Москве для квартиры размера 60 метров. 
А второй, на сколько наш прогноз отличается от конкретно выбранной случайной квартиры с площадью 60 метров. Во втором случае ошибка больше

Давайте сначала посчитаем дисперсию $\widehat{price}$ при фиксированных иксах. Расчёт:

![_](img/raschet.png)

Зная оценку дисперсии прогноза, мы можем легко оценить дисперсию ошибки прогноза в первом случае и дисперсию ошибки прогноза во втором случае. Давайте мы их сразу оценим. 

![_](img/raschet2.png)

Вот у меня получается два доверительных интервала. 

![_](img/raschet3.png)

То есть ширина доверительного интервала и ширина прогнозного интервала резко отличается. Почему? Потому что наша модель довольно хорошо точно оценивает среднюю стоимость квартиры с площадью 60 метров. И вот про среднюю стоимость квартиры в Москве, мы уверены, что она от 92 до 96. Однако если мы возьмем не некую мифическую среднестатистическую квартиру, а просто выберем наугад из всех предложений квартиру с площадью 60 метров, она, естественно, не среднестатистическая. Она может отличаться от среднестатистической как в плюс — быть дороже ее, так и быть дешевле нее. Поэтому наш доверительный интервал, наш предиктивный интервал для конкретного значения y, он оказывается шире и он соответственно равен от 94 минус 68, до 94 плюс 68.

### Интерпретация коэффициента при логарифмировании

Одним из частых преобразований при построении модели является логарифмирование.

![_](img/logariphm.png)

Рассмотрим четыре варианта комбинации нелогарифмированной переменной и логарифмированной переменной.

![_](img/ln2.png)

1. Поскольку здесь производная $y_i$ по $x_i$ в первом случае равна $\beta_2$. Это означает что при росте икса на один игрик растёт на $\beta_2$

2. $\beta_2$ показывает на сколько процентов изменится y при росте x на 1 $\%$

3. Аналагочино предыдущему пункту. 100 $\beta_2$ это изменение y в процентах при росте x на единицу в абсолютном выражении. 

4. $\frac{\beta_2}{100}$ показывает примерно изменение, изменение y в абсолютных единицах при росте x на 1 $\%$.

![_](img/udo.png)

### Дамми-переменные. Разные зависимости для подвыборок

Если объясняющая переменная (предиктор) принимает значение 1 или 0, то она называется дамми-переменной, это очень просто. Номинативная переменная.

#### Пример 1

Есть базовая модель --- Мы смотрим, как зарплата зависит от опыта работы и уровня образования.

![_](img/some_model.png)

В этой модели пол никак не учитывается, то есть мы предполагаем, что при равном опыте и при равном образовании зарплата определяется вот этой случайной составляющей $\varepsilon$, и то есть в среднем при равном опыте и при равном образовании зарплаты равны. 

#### Пример 2

Введём в модель пол --- которая равна 1 для мужчины, 0 – для женщины, для определенности. Тогда модель примет вид.

![_](img/vveli_pol.png)

Это означает, что смысл коэффициента $\beta_4$  состоит в том, на сколько при одинаковом уровне обучения и стажа работы отличается зарплата у мужчин и женщин. 

#### Пример 3

В предыдущем примере зависимость, тем не менее, влияния опыта работы на зарплату у мужчин и женщин было одинаковое. С помощью дамми-переменных легко реализовать ситуацию, где мы предполагаем, что опыт работы по-разному влияет на заработную плату у мужчин и заработную плату у женщин. 
А именно, если в наше уравнение модели, помимо, собственно, переменной male, включить переменную male помножить на experience, то что мы получим? Мы получим, что для мужчин, у них male равно 1, для них уравнение превратится в 

![_](img/pol_da.png)

Таким образом, мы с помощью дамми-переменных можем посмотреть, а правда ли, что стаж у мужчин и женщин по-разному влияет на заработную плату.

#### Пример 4

Если я поставлю дамми-переменную

![_](img/other_primer.png)

То что это означает ---  что $\beta_5$показывает, насколько год, дополнительный год обучения, по-разному влияет у мужчин и женщин. Например, если $\beta_5$ меньше 0, это означает, что дополнительный год обучения для женщин больше увеличивает заработную плату, чем для мужчин. Если $\beta_5$ положительно, то, соответственно, дополнительный год обучения сильнее увеличивает заработную плату мужчин, нежели заработную плату женщин. 

### Факторные переменные с несколькими значениями

С помощью цифр это реализуется следующим образом. Выбирается один сезон за базовый. Вот сейчас у нас зима, поэтому будем считать, что базовый сезон – это зима. И, соответственно, мы вводим 3 дамми-переменных, каждая дамми-переменная по-прежнему принимает значение 1 или 0. Мы вводим переменную «весна», которая равна 1, если весна соответствует наблюдению, и 0 иначе; переменная «лето», которая принимает значение 1, если это наблюдений летнее, и 0 иначе; и переменная «осень», которая равна 1, если наблюдение осеннее, и 0 иначе

![_](img/zima.png)

К примеру, исследователь интересуется объемом спроса на мороженое в зависимости от цены средней в киоске и сезона, в котором, значит, к которому относится наблюдение.

![_](img/icecream.png)

Соответственно, если наблюдение относится к зиме, то модель для этой части выборки, для зимних наблюдений превращается в совсем простую

![_](img/all_season.png)

$\beta_3$ – это насколько спрос на мороженое весной больше, чем спрос на мороженое зимой при той же самой цене, при фиксированной переменной price. Соответственно, $\beta_4$ показывает, насколько лето отличается от зимы. $\beta_5$ показывает, насколько осень отличается от зимы. 

#### Частая ошибка

Включать обе переменные сразу. Это неправильно

![_](img/don.png)


Неважно какую, уравнение для отдельных подвыборок получится совершенно одинаковым, неважно, какую вы включите, но если вы включите обе, то у вас возникнет жесткая линейная зависимость между регрессорами, а именно переменная male + переменная female всегда будет равняться единичке. И, соответственно, невозможно будет получить однозначные оценки метода наименьших квадратов в этом случае. Это связано с тем, что нарушена наша восьмая предпосылка. Мы говорили, что с вероятностью 1 среди регрессоров нет линейно зависимых. Если включить дамми-переменных слишком много на каждое возможное значение факторной переменной, то эта предпосылка о независимости регрессоров будет нарушена.

